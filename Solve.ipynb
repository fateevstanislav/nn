{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import itertools\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = read_csv(\"training_2140.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображение записано в виде строки чисел от 0 до 255, разделенных пробелом. Преобразуем эту строку к массиву чисел 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.Image = training.Image.apply(lambda img: np.fromstring(img, sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = training.drop(training.columns.values[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels - правильные координаты признаков, используемые для обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_labels = np.array([])\n",
    "for key_name in training.columns.values[:-1]:\n",
    "    t_labels = np.append(t_labels, np.array(training[key_name]))\n",
    "t_labels = t_labels.reshape(training.shape[1]-1, len(training.Image)).T\n",
    "t_labels = t_labels[:2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера выведем на экран размерность массива признаков в виде (кол-во строк; кол-во признаков для одного изображения).\n",
    "И также выведем признаки для первого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 30)\n",
      "[ 66.03356391  39.00227368  30.22700752  36.4216782   59.58207519\n",
      "  39.64742256  73.13034586  39.96999699  36.35657143  37.3894015\n",
      "  23.45287218  37.3894015   56.95326316  29.03364812  80.22712782\n",
      "  32.22813835  40.22760902  29.0023218   16.35637895  29.64747068\n",
      "  44.42057143  57.06680301  61.19530827  79.97016541  28.61449624\n",
      "  77.38899248  43.3126015   72.93545865  43.13070677  84.48577444]\n"
     ]
    }
   ],
   "source": [
    "print t_labels.shape\n",
    "print t_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swap_indices = [(0,2), (1,3), (4,8), (5,9), (6,10), (7,11), (12,16), (13,17), (14,18), (15,19), (22,24), (23,25)]\n",
    "def swap(l,i,j):\n",
    "    t = np.copy(l[:,i])\n",
    "    l[:,i] = l[:,j]\n",
    "    l[:,j] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для вывода изображения на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_image(data):\n",
    "    img = [[x]*3 for x in data]\n",
    "    img = np.reshape(img, (96,96,3))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для печати изображения с нанесенными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_point(img_ind, a, b, length):\n",
    "    plt.xlim([0, 96])\n",
    "    plt.ylim([96, 0])\n",
    "    f = teX[img_ind].reshape(96*96)\n",
    "    print_image(f)\n",
    "    for i in range(length):\n",
    "        plt.plot(a[img_ind][i][0],a[img_ind][i][1], 'r*')\n",
    "        plt.plot(b[img_ind][i][0],b[img_ind][i][1], 'bo')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для масштабирование признаков в прямую ((1; 96) -> (-1; 1)) и обратную сторону"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    return (x - 48) / 48\n",
    "def unscale(x):\n",
    "    return x * 48 + 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление массива образцов для обучения и тестирования, печать его размерности и первого элемента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 9216)\n",
      "[ 0.93333333  0.9254902   0.92941176 ...,  0.2745098   0.29411765\n",
      "  0.35294118]\n"
     ]
    }
   ],
   "source": [
    "t_samples = np.array([np.copy(img) for img in training.Image[:2048].apply(lambda x: x / 255.0)])\n",
    "print t_samples.shape\n",
    "print t_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def swap_samples(samples):\n",
    "    res = np.copy(samples)\n",
    "    for img in res:\n",
    "        img = img.reshape(96,96)\n",
    "        for i in range(48):\n",
    "            swap(img,i,95-i)\n",
    "        img = img.reshape(96*96,1)\n",
    "    return res\n",
    "swaped_samples = swap_samples(t_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 30)\n"
     ]
    }
   ],
   "source": [
    "def swap_labels(labels, swap_indices):\n",
    "    res = np.copy(labels)\n",
    "    for i,j in swap_indices:\n",
    "        swap(res,i,j)\n",
    "    res[:,0::2] = 96 - res[:,0::2]\n",
    "    return res\n",
    "swaped_labels = swap_labels(t_labels,swap_indices)\n",
    "print swaped_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 9216) \n",
      "(4096, 30)\n"
     ]
    }
   ],
   "source": [
    "samples = np.vstack((t_samples, swaped_samples))\n",
    "labels = np.vstack((t_labels, swaped_labels))\n",
    "print samples.shape, '\\n', labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабирование признаков и печать первого элемента до масштабирования и после"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 66.03356391  39.00227368  30.22700752  36.4216782   59.58207519\n",
      "  39.64742256  73.13034586  39.96999699  36.35657143  37.3894015\n",
      "  23.45287218  37.3894015   56.95326316  29.03364812  80.22712782\n",
      "  32.22813835  40.22760902  29.0023218   16.35637895  29.64747068\n",
      "  44.42057143  57.06680301  61.19530827  79.97016541  28.61449624\n",
      "  77.38899248  43.3126015   72.93545865  43.13070677  84.48577444]\n",
      "[ 0.37569925 -0.18745263 -0.37027068 -0.24121504  0.24129323 -0.17401203\n",
      "  0.52354887 -0.16729173 -0.24257143 -0.22105414 -0.5113985  -0.22105414\n",
      "  0.18652632 -0.39513233  0.6713985  -0.32858045 -0.16192481 -0.39578496\n",
      " -0.65924211 -0.38234436 -0.07457143  0.18889173  0.27490226  0.66604511\n",
      " -0.40386466  0.61227068 -0.09765414  0.51948872 -0.10144361  0.7601203 ]\n"
     ]
    }
   ],
   "source": [
    "print labels[0]\n",
    "labels = scale(labels)\n",
    "print labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание размеров обучающей и тестирующей выборки и вывод их на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "3584\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "full_volume = len(samples)\n",
    "train_volume = int(full_volume * 0.875)\n",
    "test_volume = full_volume - train_volume\n",
    "print \"%d\\n%d\\n%d\" % (full_volume, train_volume, test_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание обучающей и тестирующей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3584, 96, 96, 1)\n",
      "(3584, 30)\n",
      "(512, 96, 96, 1)\n",
      "(512, 30)\n"
     ]
    }
   ],
   "source": [
    "trX = np.array(samples[:train_volume], dtype=np.float)\n",
    "teX = np.array(samples[train_volume:full_volume], dtype=np.float)\n",
    "trY = np.array(labels[:train_volume], dtype=np.float)\n",
    "teY = np.array(labels[train_volume:full_volume], dtype=np.float)\n",
    "\n",
    "trX = trX.reshape(-1, 96, 96, 1)  # 96x96x1 input img\n",
    "teX = teX.reshape(-1, 96, 96, 1)  # 96x96x1 input img\n",
    "\n",
    "print trX.shape\n",
    "print trY.shape\n",
    "print teX.shape\n",
    "print teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_populations(populations):\n",
    "    configs = []\n",
    "    for p in populations:\n",
    "        if p['optimizer'] == \"RMSProp\":\n",
    "            opt = tf.train.RMSPropOptimizer\n",
    "        elif p['optimizer'] == \"Adam\":\n",
    "            opt = tf.train.AdamOptimizer\n",
    "        else:\n",
    "            opt = tf.train.AdamOptimizer\n",
    "        configs.append([opt, p['convs'], p['fc']])\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_configs(fname):\n",
    "    epochs, eps, batch_size = 10, 4, 128\n",
    "    keep_prob_conv, keep_prob_fc = 0.8, 0.5\n",
    "    configs = [[1, [3,3,3,3],[512]],\n",
    "               [0, [4,3,2,1],[700]]]\n",
    "    with open(fname) as f:\n",
    "        cf = json.load(f)\n",
    "    if cf.has_key('epochs'):\n",
    "        epochs_count = cf['epochs']\n",
    "    if cf.has_key('eps'):\n",
    "        eps = cf['eps']\n",
    "    if cf.has_key('batch_size'):\n",
    "        batch_size = cf['batch_size']\n",
    "    if cf.has_key('keep_prob_conv'):\n",
    "        keep_prob_conv = cf['keep_prob_conv']\n",
    "    if cf.has_key('keep_prob_fc'):\n",
    "        keep_prob_fc = cf['keep_prob_fc']\n",
    "    if cf.has_key('population'):\n",
    "        configs = read_populations(cf['population'])\n",
    "    return (epochs, eps, batch_size, keep_prob_conv, keep_prob_fc, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epo,eps,bs,kpc,kpf,cfgs = read_configs(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание eps для проверки точности работы нейросети, а также размеров одновременно подаваемых образцов для обучения (они подаются нейросети, вычисляется средняя ошибка для всех этих образцов и только после этого меняются веса) и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 4 # treshold = 4 pixels\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для оценки точности. Проверяется что все сравниваемые элементы отличаются не более чем на eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eq_eps(y, yl):\n",
    "    uy = unscale(y)\n",
    "    uyl = unscale(yl)\n",
    "    return np.abs(uy - uyl) < eps\n",
    "\n",
    "def all_eq_eps(y, yl):\n",
    "    return (all(eq_eps(yc, ylc)) for yc, ylc in zip(y, yl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConfigNN:\n",
    "    def __init__(self, input_img_size, output_count, opt, cost, lrn_rate,\n",
    "                 changing_learning_rate, conv_kernels, pooling_kernels,\n",
    "                 fc_layers, keep_prob_conv, keep_prob_fc,\n",
    "                 batch_size, train_volume, use_bias, epochs):\n",
    "        # img_size = 96x96, not (-1, 96, 96, 1)\n",
    "        # pooling_kernels = [...,(1,2,2,1), ...]\n",
    "        # conv_kernel = [width, height, count]\n",
    "        self.input_img_size = input_img_size # ?\n",
    "        self.output_count = output_count\n",
    "        self.optimizer = opt\n",
    "        self.train_volume = train_volume\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.changing_learning_rate = changing_learning_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.best_cost = 1000000\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        self.pooling_kernels_shapes =copy.deepcopy(pooling_kernels)\n",
    "        \n",
    "        self.keep_prob_conv = keep_prob_conv\n",
    "        self.keep_prob_fc = keep_prob_fc\n",
    "        \n",
    "        self._create_weights(conv_kernels, fc_layers)\n",
    "        \n",
    "        self.X = tf.placeholder(\"float\",[None, self.input_img_size[0], self.input_img_size[1], 1], name=\"X\")\n",
    "        self.Y = tf.placeholder(\"float\",[None, self.output_count], name=\"Y\")\n",
    "        \n",
    "        if changing_learning_rate:\n",
    "            self.global_step = tf.Variable(1, trainable=False)\n",
    "            self.learning_rate = tf.train.exponential_decay(lrn_rate, self.global_step * batch_size,\n",
    "                                           train_volume, 0.96, staircase=True)\n",
    "        else:\n",
    "            self.learning_rate = lrn_rate\n",
    "            \n",
    "        self.predict_op = self._predict(self.X)\n",
    "        self.cost = cost(self.Y - self.predict_op)\n",
    "        if changing_learning_rate:\n",
    "            self.train_op = self.optimizer(self.learning_rate).minimize(self.cost, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = self.optimizer(self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "    def print_info(self):\n",
    "        #print 'image size: ', self.input_img_size\n",
    "        #print 'outputs count: ', self.output_count\n",
    "        print 'optimizer: ', self.optimizer\n",
    "        #print 'train volume: ', self.train_volume\n",
    "        #print 'batch size: ', self.batch_size\n",
    "        print 'convolutional kernel shapes: ', self.conv_kernels_shapes\n",
    "        print 'pooling kernels shapes: ', self.pooling_kernels_shapes\n",
    "        print 'fully connected shapes: ', self.fc_layers_shapes\n",
    "        #print 'keep_prob for conv layers: ', self.keep_prob_conv\n",
    "        #print 'keep_prob for fc layers: ', self.keep_prob_fc\n",
    "        #print 'use bias: ', self.use_bias\n",
    "    \n",
    "    def _init_weights(self, shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape, stddev=0.01))\n",
    "    \n",
    "    def _create_weights(self, conv_kernels, fc_layers):\n",
    "        # предполагается что ядра свертки и субдискретизации имеют размеры NxN и их кол-во совпадает        \n",
    "        self.conv_kernels_shapes = copy.deepcopy(conv_kernels)\n",
    "        self.fc_layers_shapes = copy.deepcopy(fc_layers)\n",
    "        \n",
    "        self.conv_kernels_shapes[0][2:] = [1, self.conv_kernels_shapes[0][2]]\n",
    "        for i in range(1, len(self.conv_kernels_shapes)):\n",
    "            self.conv_kernels_shapes[i][2:] = [self.conv_kernels_shapes[i-1][-1], self.conv_kernels_shapes[i][2]]\n",
    "            \n",
    "        img_sz = self.input_img_size[0]\n",
    "        for pk in self.pooling_kernels_shapes:\n",
    "            img_sz /= pk[1]\n",
    "            \n",
    "        conv_output_count = self.conv_kernels_shapes[-1][3] * img_sz * img_sz\n",
    "        self.fc_layers_shapes[0] = [conv_output_count, self.fc_layers_shapes[0]]\n",
    "\n",
    "        for i in range(1, len(self.fc_layers_shapes)):\n",
    "            self.fc_layers_shapes[i] = [self.fc_layers_shapes[i-1][1], self.fc_layers_shapes[i]]\n",
    "\n",
    "        self.conv_kernels = [self._init_weights(sh) for sh in self.conv_kernels_shapes]\n",
    "        self.fc_layers = [self._init_weights(sh) for sh in self.fc_layers_shapes]\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.conv_biases = [tf.Variable(tf.zeros([sh[-1]])) for sh in self.conv_kernels_shapes]\n",
    "            self.fc_biases = [tf.Variable(tf.constant(0.1, shape=[sh[-1]])) for sh in self.fc_layers_shapes]\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        y = X\n",
    "        for i in range(len(self.conv_kernels)):\n",
    "            y = tf.nn.conv2d(y, self.conv_kernels[i], strides=[1,1,1,1], padding=\"SAME\")\n",
    "            if self.use_bias:\n",
    "                y = tf.nn.bias_add(y, self.conv_biases[i])\n",
    "            y = tf.nn.relu(y)\n",
    "            pooling = self.pooling_kernels_shapes[i]\n",
    "            y = tf.nn.max_pool(y, pooling, pooling, padding=\"SAME\")\n",
    "            y = tf.nn.dropout(y, self.keep_prob_conv)\n",
    "        y = tf.reshape(y, [-1, self.fc_layers_shapes[0][0]])\n",
    "        for i in range(len(self.fc_layers)-1):\n",
    "            l = self.fc_layers[i]\n",
    "            y = tf.matmul(y, l)\n",
    "            if self.use_bias:\n",
    "                y += self.fc_biases[i]\n",
    "            y = tf.nn.relu(y)\n",
    "            y = tf.nn.dropout(y, self.keep_prob_fc)\n",
    "        y = tf.matmul(y, self.fc_layers[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(cn):\n",
    "    try:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_batch = zip(range(0, cn.train_volume, cn.batch_size),\n",
    "                         range(cn.batch_size, cn.train_volume, cn.batch_size))\n",
    "\n",
    "        kp_conv = cn.keep_prob_conv\n",
    "        kp_fc = cn.keep_prob_fc\n",
    "        if not cn.changing_learning_rate:\n",
    "            cur_learning_rate = cn.learning_rate\n",
    "        shuffled_index = range(len(trX))\n",
    "            \n",
    "        for i in range(cn.epochs):\n",
    "            random.shuffle(shuffled_index)\n",
    "            trX_shuffled = [trX[j] for j in shuffled_index]\n",
    "            trY_shuffled = [trY[j] for j in shuffled_index]\n",
    "            \n",
    "            for start, end in training_batch:\n",
    "        \n",
    "                feed_dict={cn.X: trX_shuffled[start:end], cn.Y: trY_shuffled[start:end]}\n",
    "                if cn.changing_learning_rate:\n",
    "                    _, cst, cur_learning_rate = sess.run([cn.train_op, cn.cost, cn.learning_rate], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    _, cst = sess.run([cn.train_op, cn.cost], feed_dict=feed_dict)\n",
    "                \n",
    "            cn.keep_prob_conv = 1.0\n",
    "            cn.keep_prob_fc = 1.0\n",
    "            y = sess.run(cn.predict_op, feed_dict={cn.X: teX[:256]})\n",
    "            cn.keep_prob_conv = kp_conv\n",
    "            cn.keep_prob_fc = kp_fc\n",
    "\n",
    "            acc = [eq_eps(a,b) for (a,b) in zip(teY[:256], y)]\n",
    "            accuracy = np.mean(acc)\n",
    "            print \"Epoch: %d;   Cost: %7.3f;   Accuracy: %.0f%%;   Learning rate: %.5f\" % (i, cst, accuracy*100, cur_learning_rate)\n",
    "            \n",
    "            if cst < cn.best_cost:\n",
    "                cn.best_cost = cst\n",
    "                cn.best_epoch = i\n",
    "                cn.accuracy = accuracy\n",
    "            if i - cn.best_epoch > 7:\n",
    "                print \"Early stop\"\n",
    "                print \"Best cost: %7.3f;   Best epoch: %d\" % (cn.best_cost, cn.best_epoch)\n",
    "                break\n",
    "        \n",
    "        cn.keep_prob_conv = 1.0\n",
    "        cn.keep_prob_fc = 1.0\n",
    "        y = sess.run(cn.predict_op, feed_dict={cn.X: teX[:5]})\n",
    "        cn.keep_prob_conv = kp_conv\n",
    "        cn.keep_prob_fc = kp_fc\n",
    "        a, b = unscale(y[0:5]), unscale(teY[0:5])\n",
    "        length = len(a[0]) / 2\n",
    "        a = a.reshape(5, length, 2)\n",
    "        b = b.reshape(5, length, 2)\n",
    "        for i in range(5):\n",
    "            test_point(i, a, b, length)\n",
    "    except Exception as e:\n",
    "        print \"===============\\nERROR: %s\\n===============\" % e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_bool():\n",
    "    return bool(random.getrandbits(1))\n",
    "\n",
    "def mutate(conf):\n",
    "    config = copy.deepcopy(conf)\n",
    "    \n",
    "    m_conv, m_fc = random.random(), random.random()\n",
    "    config[0] = random.randint(0,1) # 0 or 1 - Adam or RMSProp\n",
    "    \n",
    "   \n",
    "    if m_conv < 0.5:\n",
    "        change = random.randint(0, 3)\n",
    "        i_conv = random.randint(0, len(config[1])-1)\n",
    "        config[1][i_conv] = random.randint(1,5)\n",
    "    \n",
    "    if m_fc < 0.5:\n",
    "        change = random.randint(0, 3)\n",
    "        i_fc = random.randint(0, len(config[2])-1)\n",
    "        if change == 0 and len(config[2]) > 1: # delete\n",
    "            del config[2][i_fc]\n",
    "        elif change == 1 and len(config[2]) < 5: # add\n",
    "            config[2][i_fc:i_fc] = [random.randint(200,1200)]\n",
    "        elif change == 2: #change value\n",
    "            config[2][i_fc] = random.randint(200,1200)\n",
    "            \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_fullshape(size, count):\n",
    "    return [size, size, count]\n",
    "\n",
    "def conv_layers_shapes(sizes):\n",
    "    conv_count = 16\n",
    "    convs = []\n",
    "    for conv_sz in sizes:\n",
    "        shape = convolution_fullshape(conv_sz, conv_count)\n",
    "        convs.append(shape)\n",
    "        conv_count *= 2\n",
    "    return convs\n",
    "\n",
    "optimizers = [tf.train.RMSPropOptimizer, tf.train.AdamOptimizer]\n",
    "\n",
    "mse_cost = lambda x: tf.reduce_mean(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "pooling2 = [1,2,2,1]\n",
    "pooling3 = [1,3,3,1]\n",
    "pooling_kernels = [pooling3, pooling2, pooling2, pooling2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#format of configuration: config[i] = [optimizer_index, [c1, c2, c3], [l1,...lk]]\n",
    "configs = [[0, [3, 3, 3, 3], [512, 512]],\n",
    "           [1, [3, 3, 3, 3], [512, 512]],\n",
    "           [0, [3, 3, 3, 3], [700]],\n",
    "           [1, [3, 3, 3, 3], [700]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_on_configs(configs):\n",
    "    results = []\n",
    "    k = 0\n",
    "    for config in configs:\n",
    "        k += 1\n",
    "        res = [None, None, None, None, config[0], config[1], config[2]]\n",
    "        conv_kernels = conv_layers_shapes(config[1])\n",
    "        opt_ind = config[0]\n",
    "        cn = ConfigNN([96,96], 30, optimizers[opt_ind], mse_cost, 0.001, True,\n",
    "                      conv_kernels=conv_kernels,\n",
    "                      pooling_kernels=pooling_kernels,\n",
    "                      fc_layers=config[2]+[30],\n",
    "                      keep_prob_conv=0.8,\n",
    "                      keep_prob_fc=0.5,\n",
    "                      batch_size=batch_size, train_volume=train_volume,\n",
    "                      use_bias=True, epochs=10)\n",
    "        print '    k = ', k\n",
    "        cn.print_info()\n",
    "        start_time = time.clock()\n",
    "        run(cn)\n",
    "        t = time.clock() - start_time\n",
    "        res[3] = t\n",
    "        #print 'Time(s): ', t\n",
    "\n",
    "        try:\n",
    "            res[0] = cn.best_epoch\n",
    "            res[1] = cn.best_cost\n",
    "            res[2] = cn.accuracy\n",
    "        except Exception as e:\n",
    "            print e\n",
    "\n",
    "        results.append(res)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_configs(configs):\n",
    "    for config in configs:\n",
    "        print config\n",
    "        \n",
    "def print_in_file(num, data):\n",
    "    fname = 'solve_genet_'+str(num)+'.csv'\n",
    "    with open(fname, 'w') as fp:\n",
    "        cw = csv.writer(fp, delimiter=';')\n",
    "        cw.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 1\n",
    "\n",
    "half = len(configs)\n",
    "m_configs = [mutate(c) for c in configs]\n",
    "configs = configs + m_configs\n",
    "\n",
    "for i in range(5):\n",
    "    num += 1\n",
    "    print_configs(configs[half:])\n",
    "    results[half:] = run_on_configs(configs[half:])\n",
    "    \n",
    "    results.sort(key=lambda x: x[1])\n",
    "    \n",
    "    print_in_file(num, results)\n",
    "    \n",
    "    configs = [res[half:] for res in results]\n",
    "    \n",
    "    m_configs = [mutate(c) for c in configs[:half]]\n",
    "    configs[half:] = m_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрытие сессии tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
